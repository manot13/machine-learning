{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, X_credit_teste, y_credit_treinamento, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74442288\n",
      "Iteration 2, loss = 0.73263677\n",
      "Iteration 3, loss = 0.72118789\n",
      "Iteration 4, loss = 0.71095949\n",
      "Iteration 5, loss = 0.70098116\n",
      "Iteration 6, loss = 0.69183747\n",
      "Iteration 7, loss = 0.68324962\n",
      "Iteration 8, loss = 0.67558485\n",
      "Iteration 9, loss = 0.66793686\n",
      "Iteration 10, loss = 0.66103385\n",
      "Iteration 11, loss = 0.65431520\n",
      "Iteration 12, loss = 0.64797914\n",
      "Iteration 13, loss = 0.64226781\n",
      "Iteration 14, loss = 0.63641844\n",
      "Iteration 15, loss = 0.63102278\n",
      "Iteration 16, loss = 0.62585471\n",
      "Iteration 17, loss = 0.62105503\n",
      "Iteration 18, loss = 0.61620807\n",
      "Iteration 19, loss = 0.61165990\n",
      "Iteration 20, loss = 0.60721879\n",
      "Iteration 21, loss = 0.60261781\n",
      "Iteration 22, loss = 0.59785301\n",
      "Iteration 23, loss = 0.59316715\n",
      "Iteration 24, loss = 0.58865267\n",
      "Iteration 25, loss = 0.58406987\n",
      "Iteration 26, loss = 0.57933187\n",
      "Iteration 27, loss = 0.57422740\n",
      "Iteration 28, loss = 0.56897917\n",
      "Iteration 29, loss = 0.56376866\n",
      "Iteration 30, loss = 0.55842914\n",
      "Iteration 31, loss = 0.55303492\n",
      "Iteration 32, loss = 0.54739074\n",
      "Iteration 33, loss = 0.54178876\n",
      "Iteration 34, loss = 0.53615803\n",
      "Iteration 35, loss = 0.53111178\n",
      "Iteration 36, loss = 0.52633996\n",
      "Iteration 37, loss = 0.52177689\n",
      "Iteration 38, loss = 0.51763862\n",
      "Iteration 39, loss = 0.51356692\n",
      "Iteration 40, loss = 0.50980172\n",
      "Iteration 41, loss = 0.50633652\n",
      "Iteration 42, loss = 0.50290479\n",
      "Iteration 43, loss = 0.49975437\n",
      "Iteration 44, loss = 0.49667725\n",
      "Iteration 45, loss = 0.49367945\n",
      "Iteration 46, loss = 0.49078575\n",
      "Iteration 47, loss = 0.48795122\n",
      "Iteration 48, loss = 0.48522840\n",
      "Iteration 49, loss = 0.48250447\n",
      "Iteration 50, loss = 0.47994280\n",
      "Iteration 51, loss = 0.47726040\n",
      "Iteration 52, loss = 0.47464634\n",
      "Iteration 53, loss = 0.47209255\n",
      "Iteration 54, loss = 0.46944852\n",
      "Iteration 55, loss = 0.46683242\n",
      "Iteration 56, loss = 0.46417962\n",
      "Iteration 57, loss = 0.46149094\n",
      "Iteration 58, loss = 0.45881469\n",
      "Iteration 59, loss = 0.45616543\n",
      "Iteration 60, loss = 0.45348360\n",
      "Iteration 61, loss = 0.45085691\n",
      "Iteration 62, loss = 0.44817764\n",
      "Iteration 63, loss = 0.44551861\n",
      "Iteration 64, loss = 0.44283790\n",
      "Iteration 65, loss = 0.43993123\n",
      "Iteration 66, loss = 0.43703768\n",
      "Iteration 67, loss = 0.43419079\n",
      "Iteration 68, loss = 0.43130089\n",
      "Iteration 69, loss = 0.42843478\n",
      "Iteration 70, loss = 0.42563014\n",
      "Iteration 71, loss = 0.42281959\n",
      "Iteration 72, loss = 0.41997860\n",
      "Iteration 73, loss = 0.41718221\n",
      "Iteration 74, loss = 0.41447945\n",
      "Iteration 75, loss = 0.41174003\n",
      "Iteration 76, loss = 0.40903563\n",
      "Iteration 77, loss = 0.40626559\n",
      "Iteration 78, loss = 0.40356551\n",
      "Iteration 79, loss = 0.40072522\n",
      "Iteration 80, loss = 0.39788660\n",
      "Iteration 81, loss = 0.39491382\n",
      "Iteration 82, loss = 0.39091060\n",
      "Iteration 83, loss = 0.38646219\n",
      "Iteration 84, loss = 0.38167227\n",
      "Iteration 85, loss = 0.37680096\n",
      "Iteration 86, loss = 0.37200420\n",
      "Iteration 87, loss = 0.36728581\n",
      "Iteration 88, loss = 0.36233133\n",
      "Iteration 89, loss = 0.35755957\n",
      "Iteration 90, loss = 0.35265312\n",
      "Iteration 91, loss = 0.34778342\n",
      "Iteration 92, loss = 0.34282863\n",
      "Iteration 93, loss = 0.33776066\n",
      "Iteration 94, loss = 0.33249767\n",
      "Iteration 95, loss = 0.32728739\n",
      "Iteration 96, loss = 0.32195852\n",
      "Iteration 97, loss = 0.31677323\n",
      "Iteration 98, loss = 0.31172278\n",
      "Iteration 99, loss = 0.30656437\n",
      "Iteration 100, loss = 0.30128155\n",
      "Iteration 101, loss = 0.29628098\n",
      "Iteration 102, loss = 0.29130966\n",
      "Iteration 103, loss = 0.28647110\n",
      "Iteration 104, loss = 0.28155317\n",
      "Iteration 105, loss = 0.27670724\n",
      "Iteration 106, loss = 0.27189529\n",
      "Iteration 107, loss = 0.26720409\n",
      "Iteration 108, loss = 0.26247191\n",
      "Iteration 109, loss = 0.25781979\n",
      "Iteration 110, loss = 0.25340053\n",
      "Iteration 111, loss = 0.24878505\n",
      "Iteration 112, loss = 0.24435732\n",
      "Iteration 113, loss = 0.23997128\n",
      "Iteration 114, loss = 0.23570056\n",
      "Iteration 115, loss = 0.23145346\n",
      "Iteration 116, loss = 0.22723830\n",
      "Iteration 117, loss = 0.22316253\n",
      "Iteration 118, loss = 0.21935231\n",
      "Iteration 119, loss = 0.21574613\n",
      "Iteration 120, loss = 0.21218432\n",
      "Iteration 121, loss = 0.20892365\n",
      "Iteration 122, loss = 0.20568735\n",
      "Iteration 123, loss = 0.20252877\n",
      "Iteration 124, loss = 0.19964906\n",
      "Iteration 125, loss = 0.19668448\n",
      "Iteration 126, loss = 0.19383711\n",
      "Iteration 127, loss = 0.19114999\n",
      "Iteration 128, loss = 0.18873185\n",
      "Iteration 129, loss = 0.18636866\n",
      "Iteration 130, loss = 0.18412629\n",
      "Iteration 131, loss = 0.18190037\n",
      "Iteration 132, loss = 0.17984209\n",
      "Iteration 133, loss = 0.17800172\n",
      "Iteration 134, loss = 0.17602821\n",
      "Iteration 135, loss = 0.17417100\n",
      "Iteration 136, loss = 0.17240302\n",
      "Iteration 137, loss = 0.17074276\n",
      "Iteration 138, loss = 0.16914735\n",
      "Iteration 139, loss = 0.16758467\n",
      "Iteration 140, loss = 0.16608963\n",
      "Iteration 141, loss = 0.16457085\n",
      "Iteration 142, loss = 0.16310718\n",
      "Iteration 143, loss = 0.16172069\n",
      "Iteration 144, loss = 0.16037334\n",
      "Iteration 145, loss = 0.15910190\n",
      "Iteration 146, loss = 0.15779279\n",
      "Iteration 147, loss = 0.15651367\n",
      "Iteration 148, loss = 0.15535848\n",
      "Iteration 149, loss = 0.15418823\n",
      "Iteration 150, loss = 0.15299022\n",
      "Iteration 151, loss = 0.15185395\n",
      "Iteration 152, loss = 0.15071510\n",
      "Iteration 153, loss = 0.14964552\n",
      "Iteration 154, loss = 0.14849213\n",
      "Iteration 155, loss = 0.14744907\n",
      "Iteration 156, loss = 0.14643278\n",
      "Iteration 157, loss = 0.14540082\n",
      "Iteration 158, loss = 0.14438950\n",
      "Iteration 159, loss = 0.14341057\n",
      "Iteration 160, loss = 0.14245325\n",
      "Iteration 161, loss = 0.14158301\n",
      "Iteration 162, loss = 0.14071585\n",
      "Iteration 163, loss = 0.13973058\n",
      "Iteration 164, loss = 0.13885562\n",
      "Iteration 165, loss = 0.13796939\n",
      "Iteration 166, loss = 0.13715744\n",
      "Iteration 167, loss = 0.13643491\n",
      "Iteration 168, loss = 0.13559041\n",
      "Iteration 169, loss = 0.13479887\n",
      "Iteration 170, loss = 0.13409825\n",
      "Iteration 171, loss = 0.13337216\n",
      "Iteration 172, loss = 0.13267310\n",
      "Iteration 173, loss = 0.13202380\n",
      "Iteration 174, loss = 0.13139262\n",
      "Iteration 175, loss = 0.13075305\n",
      "Iteration 176, loss = 0.13007798\n",
      "Iteration 177, loss = 0.12950743\n",
      "Iteration 178, loss = 0.12889492\n",
      "Iteration 179, loss = 0.12824716\n",
      "Iteration 180, loss = 0.12766751\n",
      "Iteration 181, loss = 0.12699406\n",
      "Iteration 182, loss = 0.12641696\n",
      "Iteration 183, loss = 0.12586543\n",
      "Iteration 184, loss = 0.12529252\n",
      "Iteration 185, loss = 0.12472514\n",
      "Iteration 186, loss = 0.12426925\n",
      "Iteration 187, loss = 0.12361613\n",
      "Iteration 188, loss = 0.12321622\n",
      "Iteration 189, loss = 0.12263950\n",
      "Iteration 190, loss = 0.12210325\n",
      "Iteration 191, loss = 0.12161204\n",
      "Iteration 192, loss = 0.12114339\n",
      "Iteration 193, loss = 0.12068065\n",
      "Iteration 194, loss = 0.12012860\n",
      "Iteration 195, loss = 0.11962170\n",
      "Iteration 196, loss = 0.11913199\n",
      "Iteration 197, loss = 0.11863623\n",
      "Iteration 198, loss = 0.11810177\n",
      "Iteration 199, loss = 0.11765632\n",
      "Iteration 200, loss = 0.11718144\n",
      "Iteration 201, loss = 0.11669297\n",
      "Iteration 202, loss = 0.11622486\n",
      "Iteration 203, loss = 0.11579901\n",
      "Iteration 204, loss = 0.11540361\n",
      "Iteration 205, loss = 0.11498164\n",
      "Iteration 206, loss = 0.11462449\n",
      "Iteration 207, loss = 0.11417303\n",
      "Iteration 208, loss = 0.11371594\n",
      "Iteration 209, loss = 0.11339003\n",
      "Iteration 210, loss = 0.11303702\n",
      "Iteration 211, loss = 0.11260756\n",
      "Iteration 212, loss = 0.11223727\n",
      "Iteration 213, loss = 0.11188513\n",
      "Iteration 214, loss = 0.11148630\n",
      "Iteration 215, loss = 0.11110223\n",
      "Iteration 216, loss = 0.11080865\n",
      "Iteration 217, loss = 0.11029884\n",
      "Iteration 218, loss = 0.10999107\n",
      "Iteration 219, loss = 0.10959576\n",
      "Iteration 220, loss = 0.10915677\n",
      "Iteration 221, loss = 0.10884219\n",
      "Iteration 222, loss = 0.10845436\n",
      "Iteration 223, loss = 0.10810146\n",
      "Iteration 224, loss = 0.10773377\n",
      "Iteration 225, loss = 0.10743370\n",
      "Iteration 226, loss = 0.10705944\n",
      "Iteration 227, loss = 0.10667067\n",
      "Iteration 228, loss = 0.10636129\n",
      "Iteration 229, loss = 0.10604461\n",
      "Iteration 230, loss = 0.10568078\n",
      "Iteration 231, loss = 0.10536068\n",
      "Iteration 232, loss = 0.10506092\n",
      "Iteration 233, loss = 0.10470922\n",
      "Iteration 234, loss = 0.10435913\n",
      "Iteration 235, loss = 0.10402523\n",
      "Iteration 236, loss = 0.10366076\n",
      "Iteration 237, loss = 0.10334731\n",
      "Iteration 238, loss = 0.10318373\n",
      "Iteration 239, loss = 0.10278289\n",
      "Iteration 240, loss = 0.10241339\n",
      "Iteration 241, loss = 0.10214333\n",
      "Iteration 242, loss = 0.10182310\n",
      "Iteration 243, loss = 0.10153569\n",
      "Iteration 244, loss = 0.10125524\n",
      "Iteration 245, loss = 0.10097332\n",
      "Iteration 246, loss = 0.10066930\n",
      "Iteration 247, loss = 0.10042307\n",
      "Iteration 248, loss = 0.10015173\n",
      "Iteration 249, loss = 0.09991179\n",
      "Iteration 250, loss = 0.09962718\n",
      "Iteration 251, loss = 0.09936521\n",
      "Iteration 252, loss = 0.09908454\n",
      "Iteration 253, loss = 0.09882631\n",
      "Iteration 254, loss = 0.09858237\n",
      "Iteration 255, loss = 0.09838513\n",
      "Iteration 256, loss = 0.09808337\n",
      "Iteration 257, loss = 0.09789363\n",
      "Iteration 258, loss = 0.09763217\n",
      "Iteration 259, loss = 0.09739285\n",
      "Iteration 260, loss = 0.09715062\n",
      "Iteration 261, loss = 0.09691461\n",
      "Iteration 262, loss = 0.09671217\n",
      "Iteration 263, loss = 0.09647754\n",
      "Iteration 264, loss = 0.09623910\n",
      "Iteration 265, loss = 0.09599576\n",
      "Iteration 266, loss = 0.09576090\n",
      "Iteration 267, loss = 0.09553762\n",
      "Iteration 268, loss = 0.09530356\n",
      "Iteration 269, loss = 0.09508101\n",
      "Iteration 270, loss = 0.09487039\n",
      "Iteration 271, loss = 0.09468008\n",
      "Iteration 272, loss = 0.09445270\n",
      "Iteration 273, loss = 0.09423569\n",
      "Iteration 274, loss = 0.09400269\n",
      "Iteration 275, loss = 0.09377301\n",
      "Iteration 276, loss = 0.09358750\n",
      "Iteration 277, loss = 0.09339611\n",
      "Iteration 278, loss = 0.09314482\n",
      "Iteration 279, loss = 0.09289806\n",
      "Iteration 280, loss = 0.09269820\n",
      "Iteration 281, loss = 0.09247476\n",
      "Iteration 282, loss = 0.09225989\n",
      "Iteration 283, loss = 0.09207324\n",
      "Iteration 284, loss = 0.09188097\n",
      "Iteration 285, loss = 0.09171187\n",
      "Iteration 286, loss = 0.09146802\n",
      "Iteration 287, loss = 0.09127974\n",
      "Iteration 288, loss = 0.09108198\n",
      "Iteration 289, loss = 0.09084079\n",
      "Iteration 290, loss = 0.09061491\n",
      "Iteration 291, loss = 0.09045918\n",
      "Iteration 292, loss = 0.09023210\n",
      "Iteration 293, loss = 0.09008965\n",
      "Iteration 294, loss = 0.08995822\n",
      "Iteration 295, loss = 0.08968673\n",
      "Iteration 296, loss = 0.08949593\n",
      "Iteration 297, loss = 0.08927689\n",
      "Iteration 298, loss = 0.08907070\n",
      "Iteration 299, loss = 0.08885799\n",
      "Iteration 300, loss = 0.08866430\n",
      "Iteration 301, loss = 0.08846834\n",
      "Iteration 302, loss = 0.08827876\n",
      "Iteration 303, loss = 0.08810739\n",
      "Iteration 304, loss = 0.08789753\n",
      "Iteration 305, loss = 0.08770618\n",
      "Iteration 306, loss = 0.08753178\n",
      "Iteration 307, loss = 0.08731133\n",
      "Iteration 308, loss = 0.08712795\n",
      "Iteration 309, loss = 0.08695532\n",
      "Iteration 310, loss = 0.08676361\n",
      "Iteration 311, loss = 0.08660784\n",
      "Iteration 312, loss = 0.08649753\n",
      "Iteration 313, loss = 0.08626174\n",
      "Iteration 314, loss = 0.08615688\n",
      "Iteration 315, loss = 0.08593599\n",
      "Iteration 316, loss = 0.08572513\n",
      "Iteration 317, loss = 0.08555590\n",
      "Iteration 318, loss = 0.08534604\n",
      "Iteration 319, loss = 0.08514954\n",
      "Iteration 320, loss = 0.08498034\n",
      "Iteration 321, loss = 0.08478619\n",
      "Iteration 322, loss = 0.08463563\n",
      "Iteration 323, loss = 0.08440435\n",
      "Iteration 324, loss = 0.08421990\n",
      "Iteration 325, loss = 0.08404031\n",
      "Iteration 326, loss = 0.08390500\n",
      "Iteration 327, loss = 0.08369110\n",
      "Iteration 328, loss = 0.08354489\n",
      "Iteration 329, loss = 0.08334298\n",
      "Iteration 330, loss = 0.08317865\n",
      "Iteration 331, loss = 0.08296136\n",
      "Iteration 332, loss = 0.08278331\n",
      "Iteration 333, loss = 0.08254556\n",
      "Iteration 334, loss = 0.08231146\n",
      "Iteration 335, loss = 0.08206344\n",
      "Iteration 336, loss = 0.08188026\n",
      "Iteration 337, loss = 0.08162847\n",
      "Iteration 338, loss = 0.08138518\n",
      "Iteration 339, loss = 0.08111957\n",
      "Iteration 340, loss = 0.08090666\n",
      "Iteration 341, loss = 0.08064473\n",
      "Iteration 342, loss = 0.08040335\n",
      "Iteration 343, loss = 0.08014004\n",
      "Iteration 344, loss = 0.07986381\n",
      "Iteration 345, loss = 0.07956260\n",
      "Iteration 346, loss = 0.07933288\n",
      "Iteration 347, loss = 0.07906440\n",
      "Iteration 348, loss = 0.07883014\n",
      "Iteration 349, loss = 0.07848041\n",
      "Iteration 350, loss = 0.07821831\n",
      "Iteration 351, loss = 0.07796569\n",
      "Iteration 352, loss = 0.07769936\n",
      "Iteration 353, loss = 0.07743575\n",
      "Iteration 354, loss = 0.07716592\n",
      "Iteration 355, loss = 0.07689083\n",
      "Iteration 356, loss = 0.07663435\n",
      "Iteration 357, loss = 0.07630931\n",
      "Iteration 358, loss = 0.07607753\n",
      "Iteration 359, loss = 0.07580189\n",
      "Iteration 360, loss = 0.07549499\n",
      "Iteration 361, loss = 0.07521839\n",
      "Iteration 362, loss = 0.07489642\n",
      "Iteration 363, loss = 0.07447524\n",
      "Iteration 364, loss = 0.07413376\n",
      "Iteration 365, loss = 0.07374641\n",
      "Iteration 366, loss = 0.07341771\n",
      "Iteration 367, loss = 0.07306671\n",
      "Iteration 368, loss = 0.07265278\n",
      "Iteration 369, loss = 0.07238244\n",
      "Iteration 370, loss = 0.07193329\n",
      "Iteration 371, loss = 0.07156150\n",
      "Iteration 372, loss = 0.07124738\n",
      "Iteration 373, loss = 0.07074138\n",
      "Iteration 374, loss = 0.07031272\n",
      "Iteration 375, loss = 0.06993506\n",
      "Iteration 376, loss = 0.06951357\n",
      "Iteration 377, loss = 0.06910324\n",
      "Iteration 378, loss = 0.06866303\n",
      "Iteration 379, loss = 0.06824391\n",
      "Iteration 380, loss = 0.06784571\n",
      "Iteration 381, loss = 0.06746658\n",
      "Iteration 382, loss = 0.06705414\n",
      "Iteration 383, loss = 0.06670087\n",
      "Iteration 384, loss = 0.06627435\n",
      "Iteration 385, loss = 0.06586852\n",
      "Iteration 386, loss = 0.06558759\n",
      "Iteration 387, loss = 0.06511937\n",
      "Iteration 388, loss = 0.06475859\n",
      "Iteration 389, loss = 0.06440454\n",
      "Iteration 390, loss = 0.06403596\n",
      "Iteration 391, loss = 0.06369667\n",
      "Iteration 392, loss = 0.06330830\n",
      "Iteration 393, loss = 0.06301908\n",
      "Iteration 394, loss = 0.06265471\n",
      "Iteration 395, loss = 0.06236001\n",
      "Iteration 396, loss = 0.06201438\n",
      "Iteration 397, loss = 0.06168991\n",
      "Iteration 398, loss = 0.06136435\n",
      "Iteration 399, loss = 0.06109237\n",
      "Iteration 400, loss = 0.06073985\n",
      "Iteration 401, loss = 0.06049466\n",
      "Iteration 402, loss = 0.06033085\n",
      "Iteration 403, loss = 0.05995523\n",
      "Iteration 404, loss = 0.05966034\n",
      "Iteration 405, loss = 0.05935879\n",
      "Iteration 406, loss = 0.05909499\n",
      "Iteration 407, loss = 0.05883091\n",
      "Iteration 408, loss = 0.05855984\n",
      "Iteration 409, loss = 0.05830035\n",
      "Iteration 410, loss = 0.05803456\n",
      "Iteration 411, loss = 0.05780683\n",
      "Iteration 412, loss = 0.05743850\n",
      "Iteration 413, loss = 0.05715420\n",
      "Iteration 414, loss = 0.05686266\n",
      "Iteration 415, loss = 0.05655135\n",
      "Iteration 416, loss = 0.05625049\n",
      "Iteration 417, loss = 0.05599861\n",
      "Iteration 418, loss = 0.05567440\n",
      "Iteration 419, loss = 0.05543076\n",
      "Iteration 420, loss = 0.05519929\n",
      "Iteration 421, loss = 0.05486130\n",
      "Iteration 422, loss = 0.05458627\n",
      "Iteration 423, loss = 0.05429435\n",
      "Iteration 424, loss = 0.05403391\n",
      "Iteration 425, loss = 0.05372462\n",
      "Iteration 426, loss = 0.05347025\n",
      "Iteration 427, loss = 0.05315433\n",
      "Iteration 428, loss = 0.05286575\n",
      "Iteration 429, loss = 0.05261773\n",
      "Iteration 430, loss = 0.05228452\n",
      "Iteration 431, loss = 0.05207477\n",
      "Iteration 432, loss = 0.05182959\n",
      "Iteration 433, loss = 0.05157221\n",
      "Iteration 434, loss = 0.05129948\n",
      "Iteration 435, loss = 0.05105823\n",
      "Iteration 436, loss = 0.05081969\n",
      "Iteration 437, loss = 0.05058537\n",
      "Iteration 438, loss = 0.05031262\n",
      "Iteration 439, loss = 0.05008035\n",
      "Iteration 440, loss = 0.04986314\n",
      "Iteration 441, loss = 0.04966474\n",
      "Iteration 442, loss = 0.04941574\n",
      "Iteration 443, loss = 0.04914944\n",
      "Iteration 444, loss = 0.04896126\n",
      "Iteration 445, loss = 0.04869724\n",
      "Iteration 446, loss = 0.04852833\n",
      "Iteration 447, loss = 0.04827716\n",
      "Iteration 448, loss = 0.04806675\n",
      "Iteration 449, loss = 0.04787297\n",
      "Iteration 450, loss = 0.04767118\n",
      "Iteration 451, loss = 0.04742327\n",
      "Iteration 452, loss = 0.04719929\n",
      "Iteration 453, loss = 0.04704464\n",
      "Iteration 454, loss = 0.04680177\n",
      "Iteration 455, loss = 0.04661066\n",
      "Iteration 456, loss = 0.04639145\n",
      "Iteration 457, loss = 0.04619588\n",
      "Iteration 458, loss = 0.04596325\n",
      "Iteration 459, loss = 0.04576812\n",
      "Iteration 460, loss = 0.04556499\n",
      "Iteration 461, loss = 0.04535139\n",
      "Iteration 462, loss = 0.04514843\n",
      "Iteration 463, loss = 0.04494911\n",
      "Iteration 464, loss = 0.04474062\n",
      "Iteration 465, loss = 0.04456535\n",
      "Iteration 466, loss = 0.04434628\n",
      "Iteration 467, loss = 0.04417255\n",
      "Iteration 468, loss = 0.04395887\n",
      "Iteration 469, loss = 0.04381362\n",
      "Iteration 470, loss = 0.04359139\n",
      "Iteration 471, loss = 0.04338051\n",
      "Iteration 472, loss = 0.04317795\n",
      "Iteration 473, loss = 0.04299992\n",
      "Iteration 474, loss = 0.04282213\n",
      "Iteration 475, loss = 0.04261915\n",
      "Iteration 476, loss = 0.04244511\n",
      "Iteration 477, loss = 0.04227812\n",
      "Iteration 478, loss = 0.04208732\n",
      "Iteration 479, loss = 0.04189395\n",
      "Iteration 480, loss = 0.04174136\n",
      "Iteration 481, loss = 0.04151144\n",
      "Iteration 482, loss = 0.04131294\n",
      "Iteration 483, loss = 0.04116439\n",
      "Iteration 484, loss = 0.04096450\n",
      "Iteration 485, loss = 0.04078390\n",
      "Iteration 486, loss = 0.04063867\n",
      "Iteration 487, loss = 0.04044153\n",
      "Iteration 488, loss = 0.04027716\n",
      "Iteration 489, loss = 0.04010479\n",
      "Iteration 490, loss = 0.03993218\n",
      "Iteration 491, loss = 0.03976306\n",
      "Iteration 492, loss = 0.03957644\n",
      "Iteration 493, loss = 0.03942358\n",
      "Iteration 494, loss = 0.03922730\n",
      "Iteration 495, loss = 0.03907857\n",
      "Iteration 496, loss = 0.03889328\n",
      "Iteration 497, loss = 0.03879109\n",
      "Iteration 498, loss = 0.03868178\n",
      "Iteration 499, loss = 0.03840879\n",
      "Iteration 500, loss = 0.03824273\n",
      "Iteration 501, loss = 0.03807731\n",
      "Iteration 502, loss = 0.03794087\n",
      "Iteration 503, loss = 0.03774785\n",
      "Iteration 504, loss = 0.03757588\n",
      "Iteration 505, loss = 0.03742333\n",
      "Iteration 506, loss = 0.03727878\n",
      "Iteration 507, loss = 0.03710066\n",
      "Iteration 508, loss = 0.03693801\n",
      "Iteration 509, loss = 0.03676943\n",
      "Iteration 510, loss = 0.03664306\n",
      "Iteration 511, loss = 0.03645909\n",
      "Iteration 512, loss = 0.03629818\n",
      "Iteration 513, loss = 0.03619571\n",
      "Iteration 514, loss = 0.03601875\n",
      "Iteration 515, loss = 0.03583995\n",
      "Iteration 516, loss = 0.03575511\n",
      "Iteration 517, loss = 0.03551995\n",
      "Iteration 518, loss = 0.03539406\n",
      "Iteration 519, loss = 0.03528609\n",
      "Iteration 520, loss = 0.03515400\n",
      "Iteration 521, loss = 0.03501918\n",
      "Iteration 522, loss = 0.03480939\n",
      "Iteration 523, loss = 0.03467577\n",
      "Iteration 524, loss = 0.03456210\n",
      "Iteration 525, loss = 0.03441050\n",
      "Iteration 526, loss = 0.03427011\n",
      "Iteration 527, loss = 0.03412366\n",
      "Iteration 528, loss = 0.03398451\n",
      "Iteration 529, loss = 0.03386736\n",
      "Iteration 530, loss = 0.03371717\n",
      "Iteration 531, loss = 0.03358298\n",
      "Iteration 532, loss = 0.03343501\n",
      "Iteration 533, loss = 0.03343537\n",
      "Iteration 534, loss = 0.03323036\n",
      "Iteration 535, loss = 0.03305577\n",
      "Iteration 536, loss = 0.03289841\n",
      "Iteration 537, loss = 0.03275511\n",
      "Iteration 538, loss = 0.03264911\n",
      "Iteration 539, loss = 0.03251401\n",
      "Iteration 540, loss = 0.03240121\n",
      "Iteration 541, loss = 0.03226931\n",
      "Iteration 542, loss = 0.03214354\n",
      "Iteration 543, loss = 0.03204749\n",
      "Iteration 544, loss = 0.03193107\n",
      "Iteration 545, loss = 0.03178877\n",
      "Iteration 546, loss = 0.03164436\n",
      "Iteration 547, loss = 0.03151775\n",
      "Iteration 548, loss = 0.03141967\n",
      "Iteration 549, loss = 0.03125842\n",
      "Iteration 550, loss = 0.03112999\n",
      "Iteration 551, loss = 0.03105442\n",
      "Iteration 552, loss = 0.03089385\n",
      "Iteration 553, loss = 0.03079422\n",
      "Iteration 554, loss = 0.03068166\n",
      "Iteration 555, loss = 0.03056878\n",
      "Iteration 556, loss = 0.03045176\n",
      "Iteration 557, loss = 0.03037775\n",
      "Iteration 558, loss = 0.03024854\n",
      "Iteration 559, loss = 0.03018697\n",
      "Iteration 560, loss = 0.02997859\n",
      "Iteration 561, loss = 0.02985906\n",
      "Iteration 562, loss = 0.02974773\n",
      "Iteration 563, loss = 0.02964858\n",
      "Iteration 564, loss = 0.02957007\n",
      "Iteration 565, loss = 0.02941812\n",
      "Iteration 566, loss = 0.02939709\n",
      "Iteration 567, loss = 0.02922376\n",
      "Iteration 568, loss = 0.02915133\n",
      "Iteration 569, loss = 0.02900521\n",
      "Iteration 570, loss = 0.02896364\n",
      "Iteration 571, loss = 0.02875631\n",
      "Iteration 572, loss = 0.02870017\n",
      "Iteration 573, loss = 0.02860414\n",
      "Iteration 574, loss = 0.02848251\n",
      "Iteration 575, loss = 0.02834429\n",
      "Iteration 576, loss = 0.02826343\n",
      "Iteration 577, loss = 0.02817890\n",
      "Iteration 578, loss = 0.02806030\n",
      "Iteration 579, loss = 0.02796251\n",
      "Iteration 580, loss = 0.02787749\n",
      "Iteration 581, loss = 0.02779299\n",
      "Iteration 582, loss = 0.02763484\n",
      "Iteration 583, loss = 0.02756457\n",
      "Iteration 584, loss = 0.02747177\n",
      "Iteration 585, loss = 0.02736227\n",
      "Iteration 586, loss = 0.02727118\n",
      "Iteration 587, loss = 0.02719835\n",
      "Iteration 588, loss = 0.02706110\n",
      "Iteration 589, loss = 0.02698853\n",
      "Iteration 590, loss = 0.02689102\n",
      "Iteration 591, loss = 0.02678672\n",
      "Iteration 592, loss = 0.02669034\n",
      "Iteration 593, loss = 0.02659039\n",
      "Iteration 594, loss = 0.02654088\n",
      "Iteration 595, loss = 0.02642001\n",
      "Iteration 596, loss = 0.02632764\n",
      "Iteration 597, loss = 0.02624885\n",
      "Iteration 598, loss = 0.02621392\n",
      "Iteration 599, loss = 0.02608046\n",
      "Iteration 600, loss = 0.02600863\n",
      "Iteration 601, loss = 0.02591403\n",
      "Iteration 602, loss = 0.02581376\n",
      "Iteration 603, loss = 0.02576907\n",
      "Iteration 604, loss = 0.02563992\n",
      "Iteration 605, loss = 0.02554500\n",
      "Iteration 606, loss = 0.02544796\n",
      "Iteration 607, loss = 0.02540703\n",
      "Iteration 608, loss = 0.02529655\n",
      "Iteration 609, loss = 0.02520683\n",
      "Iteration 610, loss = 0.02512474\n",
      "Iteration 611, loss = 0.02505701\n",
      "Iteration 612, loss = 0.02498513\n",
      "Iteration 613, loss = 0.02487496\n",
      "Iteration 614, loss = 0.02478881\n",
      "Iteration 615, loss = 0.02472512\n",
      "Iteration 616, loss = 0.02463257\n",
      "Iteration 617, loss = 0.02457253\n",
      "Iteration 618, loss = 0.02449975\n",
      "Iteration 619, loss = 0.02438797\n",
      "Iteration 620, loss = 0.02433202\n",
      "Iteration 621, loss = 0.02425852\n",
      "Iteration 622, loss = 0.02416195\n",
      "Iteration 623, loss = 0.02408957\n",
      "Iteration 624, loss = 0.02405029\n",
      "Iteration 625, loss = 0.02394688\n",
      "Iteration 626, loss = 0.02385166\n",
      "Iteration 627, loss = 0.02379340\n",
      "Iteration 628, loss = 0.02371036\n",
      "Iteration 629, loss = 0.02364275\n",
      "Iteration 630, loss = 0.02353518\n",
      "Iteration 631, loss = 0.02348321\n",
      "Iteration 632, loss = 0.02339688\n",
      "Iteration 633, loss = 0.02334115\n",
      "Iteration 634, loss = 0.02326646\n",
      "Iteration 635, loss = 0.02319185\n",
      "Iteration 636, loss = 0.02313069\n",
      "Iteration 637, loss = 0.02304546\n",
      "Iteration 638, loss = 0.02298665\n",
      "Iteration 639, loss = 0.02289794\n",
      "Iteration 640, loss = 0.02282880\n",
      "Iteration 641, loss = 0.02275504\n",
      "Iteration 642, loss = 0.02270296\n",
      "Iteration 643, loss = 0.02263596\n",
      "Iteration 644, loss = 0.02259285\n",
      "Iteration 645, loss = 0.02249579\n",
      "Iteration 646, loss = 0.02243458\n",
      "Iteration 647, loss = 0.02235024\n",
      "Iteration 648, loss = 0.02229590\n",
      "Iteration 649, loss = 0.02229315\n",
      "Iteration 650, loss = 0.02214180\n",
      "Iteration 651, loss = 0.02206491\n",
      "Iteration 652, loss = 0.02199848\n",
      "Iteration 653, loss = 0.02203738\n",
      "Iteration 654, loss = 0.02194185\n",
      "Iteration 655, loss = 0.02181839\n",
      "Iteration 656, loss = 0.02177351\n",
      "Iteration 657, loss = 0.02170271\n",
      "Iteration 658, loss = 0.02167024\n",
      "Iteration 659, loss = 0.02155853\n",
      "Iteration 660, loss = 0.02151458\n",
      "Iteration 661, loss = 0.02144487\n",
      "Iteration 662, loss = 0.02138742\n",
      "Iteration 663, loss = 0.02134929\n",
      "Iteration 664, loss = 0.02129283\n",
      "Iteration 665, loss = 0.02121493\n",
      "Iteration 666, loss = 0.02123669\n",
      "Iteration 667, loss = 0.02110812\n",
      "Iteration 668, loss = 0.02103737\n",
      "Iteration 669, loss = 0.02098026\n",
      "Iteration 670, loss = 0.02090220\n",
      "Iteration 671, loss = 0.02083256\n",
      "Iteration 672, loss = 0.02078929\n",
      "Iteration 673, loss = 0.02073200\n",
      "Iteration 674, loss = 0.02068721\n",
      "Iteration 675, loss = 0.02058804\n",
      "Iteration 676, loss = 0.02056787\n",
      "Iteration 677, loss = 0.02055328\n",
      "Iteration 678, loss = 0.02045260\n",
      "Iteration 679, loss = 0.02043390\n",
      "Iteration 680, loss = 0.02033565\n",
      "Iteration 681, loss = 0.02029546\n",
      "Iteration 682, loss = 0.02023978\n",
      "Iteration 683, loss = 0.02015156\n",
      "Iteration 684, loss = 0.02015483\n",
      "Iteration 685, loss = 0.02009277\n",
      "Iteration 686, loss = 0.02002544\n",
      "Iteration 687, loss = 0.01996369\n",
      "Iteration 688, loss = 0.01990089\n",
      "Iteration 689, loss = 0.01985366\n",
      "Iteration 690, loss = 0.01979227\n",
      "Iteration 691, loss = 0.01974987\n",
      "Iteration 692, loss = 0.01971192\n",
      "Iteration 693, loss = 0.01964249\n",
      "Iteration 694, loss = 0.01963803\n",
      "Iteration 695, loss = 0.01956260\n",
      "Iteration 696, loss = 0.01951553\n",
      "Iteration 697, loss = 0.01944661\n",
      "Iteration 698, loss = 0.01938536\n",
      "Iteration 699, loss = 0.01933365\n",
      "Iteration 700, loss = 0.01932557\n",
      "Iteration 701, loss = 0.01927648\n",
      "Iteration 702, loss = 0.01924760\n",
      "Iteration 703, loss = 0.01916062\n",
      "Iteration 704, loss = 0.01909668\n",
      "Iteration 705, loss = 0.01904208\n",
      "Iteration 706, loss = 0.01899488\n",
      "Iteration 707, loss = 0.01896481\n",
      "Iteration 708, loss = 0.01893574\n",
      "Iteration 709, loss = 0.01887576\n",
      "Iteration 710, loss = 0.01885799\n",
      "Iteration 711, loss = 0.01878464\n",
      "Iteration 712, loss = 0.01876716\n",
      "Iteration 713, loss = 0.01876321\n",
      "Iteration 714, loss = 0.01862063\n",
      "Iteration 715, loss = 0.01858326\n",
      "Iteration 716, loss = 0.01855733\n",
      "Iteration 717, loss = 0.01850841\n",
      "Iteration 718, loss = 0.01843579\n",
      "Iteration 719, loss = 0.01838482\n",
      "Iteration 720, loss = 0.01836138\n",
      "Iteration 721, loss = 0.01833877\n",
      "Iteration 722, loss = 0.01828389\n",
      "Iteration 723, loss = 0.01819541\n",
      "Iteration 724, loss = 0.01826335\n",
      "Iteration 725, loss = 0.01813200\n",
      "Iteration 726, loss = 0.01809586\n",
      "Iteration 727, loss = 0.01811830\n",
      "Iteration 728, loss = 0.01801032\n",
      "Iteration 729, loss = 0.01795304\n",
      "Iteration 730, loss = 0.01794415\n",
      "Iteration 731, loss = 0.01790413\n",
      "Iteration 732, loss = 0.01781683\n",
      "Iteration 733, loss = 0.01776154\n",
      "Iteration 734, loss = 0.01772107\n",
      "Iteration 735, loss = 0.01770107\n",
      "Iteration 736, loss = 0.01764874\n",
      "Iteration 737, loss = 0.01761391\n",
      "Iteration 738, loss = 0.01757059\n",
      "Iteration 739, loss = 0.01758371\n",
      "Iteration 740, loss = 0.01749305\n",
      "Iteration 741, loss = 0.01745628\n",
      "Iteration 742, loss = 0.01740496\n",
      "Iteration 743, loss = 0.01740071\n",
      "Iteration 744, loss = 0.01732920\n",
      "Iteration 745, loss = 0.01732495\n",
      "Iteration 746, loss = 0.01720506\n",
      "Iteration 747, loss = 0.01723397\n",
      "Iteration 748, loss = 0.01716795\n",
      "Iteration 749, loss = 0.01710670\n",
      "Iteration 750, loss = 0.01711369\n",
      "Iteration 751, loss = 0.01703579\n",
      "Iteration 752, loss = 0.01701928\n",
      "Iteration 753, loss = 0.01694725\n",
      "Iteration 754, loss = 0.01692459\n",
      "Iteration 755, loss = 0.01691865\n",
      "Iteration 756, loss = 0.01687164\n",
      "Iteration 757, loss = 0.01684109\n",
      "Iteration 758, loss = 0.01682767\n",
      "Iteration 759, loss = 0.01690186\n",
      "Iteration 760, loss = 0.01668288\n",
      "Iteration 761, loss = 0.01671104\n",
      "Iteration 762, loss = 0.01668675\n",
      "Iteration 763, loss = 0.01662209\n",
      "Iteration 764, loss = 0.01656670\n",
      "Iteration 765, loss = 0.01651388\n",
      "Iteration 766, loss = 0.01650302\n",
      "Iteration 767, loss = 0.01645980\n",
      "Iteration 768, loss = 0.01639772\n",
      "Iteration 769, loss = 0.01642210\n",
      "Iteration 770, loss = 0.01634872\n",
      "Iteration 771, loss = 0.01630722\n",
      "Iteration 772, loss = 0.01625029\n",
      "Iteration 773, loss = 0.01625299\n",
      "Iteration 774, loss = 0.01619560\n",
      "Iteration 775, loss = 0.01616450\n",
      "Iteration 776, loss = 0.01617812\n",
      "Iteration 777, loss = 0.01611520\n",
      "Iteration 778, loss = 0.01605730\n",
      "Iteration 779, loss = 0.01603502\n",
      "Iteration 780, loss = 0.01596381\n",
      "Iteration 781, loss = 0.01594693\n",
      "Iteration 782, loss = 0.01592475\n",
      "Iteration 783, loss = 0.01588769\n",
      "Iteration 784, loss = 0.01587612\n",
      "Iteration 785, loss = 0.01582299\n",
      "Iteration 786, loss = 0.01576969\n",
      "Iteration 787, loss = 0.01578674\n",
      "Iteration 788, loss = 0.01572360\n",
      "Iteration 789, loss = 0.01568538\n",
      "Iteration 790, loss = 0.01563936\n",
      "Iteration 791, loss = 0.01561529\n",
      "Iteration 792, loss = 0.01561611\n",
      "Iteration 793, loss = 0.01560018\n",
      "Iteration 794, loss = 0.01553372\n",
      "Iteration 795, loss = 0.01558140\n",
      "Iteration 796, loss = 0.01546059\n",
      "Iteration 797, loss = 0.01545367\n",
      "Iteration 798, loss = 0.01540462\n",
      "Iteration 799, loss = 0.01536837\n",
      "Iteration 800, loss = 0.01541763\n",
      "Iteration 801, loss = 0.01533961\n",
      "Iteration 802, loss = 0.01524698\n",
      "Iteration 803, loss = 0.01524258\n",
      "Iteration 804, loss = 0.01520795\n",
      "Iteration 805, loss = 0.01520973\n",
      "Iteration 806, loss = 0.01519088\n",
      "Iteration 807, loss = 0.01512671\n",
      "Iteration 808, loss = 0.01510625\n",
      "Iteration 809, loss = 0.01508427\n",
      "Iteration 810, loss = 0.01510212\n",
      "Iteration 811, loss = 0.01506480\n",
      "Iteration 812, loss = 0.01498024\n",
      "Iteration 813, loss = 0.01494407\n",
      "Iteration 814, loss = 0.01491125\n",
      "Iteration 815, loss = 0.01488845\n",
      "Iteration 816, loss = 0.01487740\n",
      "Iteration 817, loss = 0.01484290\n",
      "Iteration 818, loss = 0.01483837\n",
      "Iteration 819, loss = 0.01474836\n",
      "Iteration 820, loss = 0.01473192\n",
      "Iteration 821, loss = 0.01484440\n",
      "Iteration 822, loss = 0.01469649\n",
      "Iteration 823, loss = 0.01470524\n",
      "Iteration 824, loss = 0.01463740\n",
      "Iteration 825, loss = 0.01458725\n",
      "Iteration 826, loss = 0.01456888\n",
      "Iteration 827, loss = 0.01453554\n",
      "Iteration 828, loss = 0.01452955\n",
      "Iteration 829, loss = 0.01448476\n",
      "Iteration 830, loss = 0.01452126\n",
      "Iteration 831, loss = 0.01443978\n",
      "Iteration 832, loss = 0.01442680\n",
      "Iteration 833, loss = 0.01442285\n",
      "Iteration 834, loss = 0.01439020\n",
      "Iteration 835, loss = 0.01430268\n",
      "Iteration 836, loss = 0.01429034\n",
      "Iteration 837, loss = 0.01424807\n",
      "Iteration 838, loss = 0.01421292\n",
      "Iteration 839, loss = 0.01424059\n",
      "Iteration 840, loss = 0.01418813\n",
      "Iteration 841, loss = 0.01415723\n",
      "Iteration 842, loss = 0.01411265\n",
      "Iteration 843, loss = 0.01411876\n",
      "Iteration 844, loss = 0.01406169\n",
      "Iteration 845, loss = 0.01401945\n",
      "Iteration 846, loss = 0.01408385\n",
      "Iteration 847, loss = 0.01399755\n",
      "Iteration 848, loss = 0.01396421\n",
      "Iteration 849, loss = 0.01393674\n",
      "Iteration 850, loss = 0.01395621\n",
      "Iteration 851, loss = 0.01387082\n",
      "Iteration 852, loss = 0.01385911\n",
      "Iteration 853, loss = 0.01383503\n",
      "Iteration 854, loss = 0.01383580\n",
      "Iteration 855, loss = 0.01377514\n",
      "Iteration 856, loss = 0.01374910\n",
      "Iteration 857, loss = 0.01372415\n",
      "Iteration 858, loss = 0.01371617\n",
      "Iteration 859, loss = 0.01366371\n",
      "Iteration 860, loss = 0.01364374\n",
      "Iteration 861, loss = 0.01366703\n",
      "Iteration 862, loss = 0.01362260\n",
      "Iteration 863, loss = 0.01359054\n",
      "Iteration 864, loss = 0.01358254\n",
      "Iteration 865, loss = 0.01354244\n",
      "Iteration 866, loss = 0.01348894\n",
      "Iteration 867, loss = 0.01356897\n",
      "Iteration 868, loss = 0.01348532\n",
      "Iteration 869, loss = 0.01345432\n",
      "Iteration 870, loss = 0.01341013\n",
      "Iteration 871, loss = 0.01340933\n",
      "Iteration 872, loss = 0.01339922\n",
      "Iteration 873, loss = 0.01333495\n",
      "Iteration 874, loss = 0.01333894\n",
      "Iteration 875, loss = 0.01332507\n",
      "Iteration 876, loss = 0.01326242\n",
      "Iteration 877, loss = 0.01324442\n",
      "Iteration 878, loss = 0.01328651\n",
      "Iteration 879, loss = 0.01319163\n",
      "Iteration 880, loss = 0.01317300\n",
      "Iteration 881, loss = 0.01316675\n",
      "Iteration 882, loss = 0.01312748\n",
      "Iteration 883, loss = 0.01309881\n",
      "Iteration 884, loss = 0.01308003\n",
      "Iteration 885, loss = 0.01308064\n",
      "Iteration 886, loss = 0.01303861\n",
      "Iteration 887, loss = 0.01302623\n",
      "Iteration 888, loss = 0.01299539\n",
      "Iteration 889, loss = 0.01299184\n",
      "Iteration 890, loss = 0.01297460\n",
      "Iteration 891, loss = 0.01296173\n",
      "Iteration 892, loss = 0.01292889\n",
      "Iteration 893, loss = 0.01287666\n",
      "Iteration 894, loss = 0.01285322\n",
      "Iteration 895, loss = 0.01281728\n",
      "Iteration 896, loss = 0.01284043\n",
      "Iteration 897, loss = 0.01282069\n",
      "Iteration 898, loss = 0.01277637\n",
      "Iteration 899, loss = 0.01275332\n",
      "Iteration 900, loss = 0.01274387\n",
      "Iteration 901, loss = 0.01268325\n",
      "Iteration 902, loss = 0.01267707\n",
      "Iteration 903, loss = 0.01268894\n",
      "Iteration 904, loss = 0.01267405\n",
      "Iteration 905, loss = 0.01264669\n",
      "Iteration 906, loss = 0.01260674\n",
      "Iteration 907, loss = 0.01262748\n",
      "Iteration 908, loss = 0.01255132\n",
      "Iteration 909, loss = 0.01251675\n",
      "Iteration 910, loss = 0.01252849\n",
      "Iteration 911, loss = 0.01250483\n",
      "Iteration 912, loss = 0.01250666\n",
      "Iteration 913, loss = 0.01247477\n",
      "Iteration 914, loss = 0.01244702\n",
      "Iteration 915, loss = 0.01240513\n",
      "Iteration 916, loss = 0.01237350\n",
      "Iteration 917, loss = 0.01236614\n",
      "Iteration 918, loss = 0.01234374\n",
      "Iteration 919, loss = 0.01235026\n",
      "Iteration 920, loss = 0.01231144\n",
      "Iteration 921, loss = 0.01230418\n",
      "Iteration 922, loss = 0.01227211\n",
      "Iteration 923, loss = 0.01224669\n",
      "Iteration 924, loss = 0.01223084\n",
      "Iteration 925, loss = 0.01221064\n",
      "Iteration 926, loss = 0.01217307\n",
      "Iteration 927, loss = 0.01216205\n",
      "Iteration 928, loss = 0.01216698\n",
      "Iteration 929, loss = 0.01215618\n",
      "Iteration 930, loss = 0.01213880\n",
      "Iteration 931, loss = 0.01212006\n",
      "Iteration 932, loss = 0.01210786\n",
      "Iteration 933, loss = 0.01207034\n",
      "Iteration 934, loss = 0.01210318\n",
      "Iteration 935, loss = 0.01203503\n",
      "Iteration 936, loss = 0.01199900\n",
      "Iteration 937, loss = 0.01200075\n",
      "Iteration 938, loss = 0.01194866\n",
      "Iteration 939, loss = 0.01196820\n",
      "Iteration 940, loss = 0.01193139\n",
      "Iteration 941, loss = 0.01191139\n",
      "Iteration 942, loss = 0.01192086\n",
      "Iteration 943, loss = 0.01192146\n",
      "Iteration 944, loss = 0.01187432\n",
      "Iteration 945, loss = 0.01190031\n",
      "Iteration 946, loss = 0.01182210\n",
      "Iteration 947, loss = 0.01181721\n",
      "Iteration 948, loss = 0.01179158\n",
      "Iteration 949, loss = 0.01176460\n",
      "Iteration 950, loss = 0.01175673\n",
      "Iteration 951, loss = 0.01172043\n",
      "Iteration 952, loss = 0.01169701\n",
      "Iteration 953, loss = 0.01172069\n",
      "Iteration 954, loss = 0.01179830\n",
      "Iteration 955, loss = 0.01167253\n",
      "Iteration 956, loss = 0.01162687\n",
      "Iteration 957, loss = 0.01162342\n",
      "Iteration 958, loss = 0.01160598\n",
      "Iteration 959, loss = 0.01156154\n",
      "Iteration 960, loss = 0.01159786\n",
      "Iteration 961, loss = 0.01157241\n",
      "Iteration 962, loss = 0.01158522\n",
      "Iteration 963, loss = 0.01159534\n",
      "Iteration 964, loss = 0.01154352\n",
      "Iteration 965, loss = 0.01148358\n",
      "Iteration 966, loss = 0.01146673\n",
      "Iteration 967, loss = 0.01140549\n",
      "Iteration 968, loss = 0.01145374\n",
      "Iteration 969, loss = 0.01143882\n",
      "Iteration 970, loss = 0.01138467\n",
      "Iteration 971, loss = 0.01137625\n",
      "Iteration 972, loss = 0.01133917\n",
      "Iteration 973, loss = 0.01132399\n",
      "Iteration 974, loss = 0.01132422\n",
      "Iteration 975, loss = 0.01128290\n",
      "Iteration 976, loss = 0.01134648\n",
      "Iteration 977, loss = 0.01129372\n",
      "Iteration 978, loss = 0.01126038\n",
      "Iteration 979, loss = 0.01121513\n",
      "Iteration 980, loss = 0.01122579\n",
      "Iteration 981, loss = 0.01119322\n",
      "Iteration 982, loss = 0.01120619\n",
      "Iteration 983, loss = 0.01116758\n",
      "Iteration 984, loss = 0.01118469\n",
      "Iteration 985, loss = 0.01115347\n",
      "Iteration 986, loss = 0.01110253\n",
      "Iteration 987, loss = 0.01113329\n",
      "Iteration 988, loss = 0.01109375\n",
      "Iteration 989, loss = 0.01106117\n",
      "Iteration 990, loss = 0.01103135\n",
      "Iteration 991, loss = 0.01101807\n",
      "Iteration 992, loss = 0.01102673\n",
      "Iteration 993, loss = 0.01104491\n",
      "Iteration 994, loss = 0.01100500\n",
      "Iteration 995, loss = 0.01102918\n",
      "Iteration 996, loss = 0.01095187\n",
      "Iteration 997, loss = 0.01101255\n",
      "Iteration 998, loss = 0.01094367\n",
      "Iteration 999, loss = 0.01092466\n",
      "Iteration 1000, loss = 0.01090398\n",
      "Iteration 1001, loss = 0.01084519\n",
      "Iteration 1002, loss = 0.01084920\n",
      "Iteration 1003, loss = 0.01086600\n",
      "Iteration 1004, loss = 0.01084935\n",
      "Iteration 1005, loss = 0.01083043\n",
      "Iteration 1006, loss = 0.01085324\n",
      "Iteration 1007, loss = 0.01078412\n",
      "Iteration 1008, loss = 0.01079327\n",
      "Iteration 1009, loss = 0.01075589\n",
      "Iteration 1010, loss = 0.01080336\n",
      "Iteration 1011, loss = 0.01073913\n",
      "Iteration 1012, loss = 0.01073882\n",
      "Iteration 1013, loss = 0.01068436\n",
      "Iteration 1014, loss = 0.01075386\n",
      "Iteration 1015, loss = 0.01069685\n",
      "Iteration 1016, loss = 0.01065611\n",
      "Iteration 1017, loss = 0.01065022\n",
      "Iteration 1018, loss = 0.01064812\n",
      "Iteration 1019, loss = 0.01069648\n",
      "Iteration 1020, loss = 0.01067306\n",
      "Iteration 1021, loss = 0.01059964\n",
      "Iteration 1022, loss = 0.01056530\n",
      "Iteration 1023, loss = 0.01056076\n",
      "Iteration 1024, loss = 0.01055321\n",
      "Iteration 1025, loss = 0.01056182\n",
      "Iteration 1026, loss = 0.01049808\n",
      "Iteration 1027, loss = 0.01048015\n",
      "Iteration 1028, loss = 0.01047026\n",
      "Iteration 1029, loss = 0.01044984\n",
      "Iteration 1030, loss = 0.01044250\n",
      "Iteration 1031, loss = 0.01042098\n",
      "Iteration 1032, loss = 0.01040805\n",
      "Iteration 1033, loss = 0.01041849\n",
      "Iteration 1034, loss = 0.01042276\n",
      "Iteration 1035, loss = 0.01040693\n",
      "Iteration 1036, loss = 0.01039658\n",
      "Iteration 1037, loss = 0.01031786\n",
      "Iteration 1038, loss = 0.01031301\n",
      "Iteration 1039, loss = 0.01029665\n",
      "Iteration 1040, loss = 0.01029878\n",
      "Iteration 1041, loss = 0.01029115\n",
      "Iteration 1042, loss = 0.01024350\n",
      "Iteration 1043, loss = 0.01022828\n",
      "Iteration 1044, loss = 0.01027326\n",
      "Iteration 1045, loss = 0.01026612\n",
      "Iteration 1046, loss = 0.01023549\n",
      "Iteration 1047, loss = 0.01017854\n",
      "Iteration 1048, loss = 0.01017168\n",
      "Iteration 1049, loss = 0.01013404\n",
      "Iteration 1050, loss = 0.01014876\n",
      "Iteration 1051, loss = 0.01019881\n",
      "Iteration 1052, loss = 0.01012223\n",
      "Iteration 1053, loss = 0.01010273\n",
      "Iteration 1054, loss = 0.01010207\n",
      "Iteration 1055, loss = 0.01011559\n",
      "Iteration 1056, loss = 0.01006196\n",
      "Iteration 1057, loss = 0.01006349\n",
      "Iteration 1058, loss = 0.01003963\n",
      "Iteration 1059, loss = 0.01005267\n",
      "Iteration 1060, loss = 0.01005063\n",
      "Iteration 1061, loss = 0.01001234\n",
      "Iteration 1062, loss = 0.00999654\n",
      "Iteration 1063, loss = 0.01005055\n",
      "Iteration 1064, loss = 0.00997540\n",
      "Iteration 1065, loss = 0.00995801\n",
      "Iteration 1066, loss = 0.00994631\n",
      "Iteration 1067, loss = 0.00993313\n",
      "Iteration 1068, loss = 0.00991680\n",
      "Iteration 1069, loss = 0.00988940\n",
      "Iteration 1070, loss = 0.00988287\n",
      "Iteration 1071, loss = 0.00986318\n",
      "Iteration 1072, loss = 0.00986192\n",
      "Iteration 1073, loss = 0.00985944\n",
      "Iteration 1074, loss = 0.00983096\n",
      "Iteration 1075, loss = 0.00992400\n",
      "Iteration 1076, loss = 0.00977536\n",
      "Iteration 1077, loss = 0.00978674\n",
      "Iteration 1078, loss = 0.00980717\n",
      "Iteration 1079, loss = 0.00978683\n",
      "Iteration 1080, loss = 0.00977245\n",
      "Iteration 1081, loss = 0.00975642\n",
      "Iteration 1082, loss = 0.00971458\n",
      "Iteration 1083, loss = 0.00974673\n",
      "Iteration 1084, loss = 0.00970496\n",
      "Iteration 1085, loss = 0.00972406\n",
      "Iteration 1086, loss = 0.00970860\n",
      "Iteration 1087, loss = 0.00968857\n",
      "Iteration 1088, loss = 0.00974745\n",
      "Iteration 1089, loss = 0.00966063\n",
      "Iteration 1090, loss = 0.00962522\n",
      "Iteration 1091, loss = 0.00968635\n",
      "Iteration 1092, loss = 0.00962695\n",
      "Iteration 1093, loss = 0.00958458\n",
      "Iteration 1094, loss = 0.00963942\n",
      "Iteration 1095, loss = 0.00963349\n",
      "Iteration 1096, loss = 0.00968411\n",
      "Iteration 1097, loss = 0.00956519\n",
      "Iteration 1098, loss = 0.00953172\n",
      "Iteration 1099, loss = 0.00952616\n",
      "Iteration 1100, loss = 0.00952410\n",
      "Iteration 1101, loss = 0.00952416\n",
      "Iteration 1102, loss = 0.00949512\n",
      "Iteration 1103, loss = 0.00949637\n",
      "Iteration 1104, loss = 0.00950401\n",
      "Iteration 1105, loss = 0.00950859\n",
      "Iteration 1106, loss = 0.00945717\n",
      "Iteration 1107, loss = 0.00951915\n",
      "Iteration 1108, loss = 0.00942337\n",
      "Iteration 1109, loss = 0.00939882\n",
      "Iteration 1110, loss = 0.00941687\n",
      "Iteration 1111, loss = 0.00935818\n",
      "Iteration 1112, loss = 0.00943550\n",
      "Iteration 1113, loss = 0.00944559\n",
      "Iteration 1114, loss = 0.00945586\n",
      "Iteration 1115, loss = 0.00937022\n",
      "Iteration 1116, loss = 0.00932458\n",
      "Iteration 1117, loss = 0.00930987\n",
      "Iteration 1118, loss = 0.00933055\n",
      "Iteration 1119, loss = 0.00929901\n",
      "Iteration 1120, loss = 0.00928918\n",
      "Iteration 1121, loss = 0.00925664\n",
      "Iteration 1122, loss = 0.00924943\n",
      "Iteration 1123, loss = 0.00924697\n",
      "Iteration 1124, loss = 0.00926118\n",
      "Iteration 1125, loss = 0.00921235\n",
      "Iteration 1126, loss = 0.00920858\n",
      "Iteration 1127, loss = 0.00923257\n",
      "Iteration 1128, loss = 0.00919054\n",
      "Iteration 1129, loss = 0.00922709\n",
      "Iteration 1130, loss = 0.00928549\n",
      "Iteration 1131, loss = 0.00919652\n",
      "Iteration 1132, loss = 0.00927817\n",
      "Iteration 1133, loss = 0.00912703\n",
      "Iteration 1134, loss = 0.00913001\n",
      "Iteration 1135, loss = 0.00913172\n",
      "Iteration 1136, loss = 0.00911754\n",
      "Iteration 1137, loss = 0.00907399\n",
      "Iteration 1138, loss = 0.00909891\n",
      "Iteration 1139, loss = 0.00910995\n",
      "Iteration 1140, loss = 0.00906566\n",
      "Iteration 1141, loss = 0.00903768\n",
      "Iteration 1142, loss = 0.00903195\n",
      "Iteration 1143, loss = 0.00905788\n",
      "Iteration 1144, loss = 0.00904124\n",
      "Iteration 1145, loss = 0.00901161\n",
      "Iteration 1146, loss = 0.00899287\n",
      "Iteration 1147, loss = 0.00902197\n",
      "Iteration 1148, loss = 0.00898774\n",
      "Iteration 1149, loss = 0.00902975\n",
      "Iteration 1150, loss = 0.00894427\n",
      "Iteration 1151, loss = 0.00895377\n",
      "Iteration 1152, loss = 0.00891619\n",
      "Iteration 1153, loss = 0.00892703\n",
      "Iteration 1154, loss = 0.00897419\n",
      "Iteration 1155, loss = 0.00892109\n",
      "Iteration 1156, loss = 0.00889197\n",
      "Iteration 1157, loss = 0.00887341\n",
      "Iteration 1158, loss = 0.00887824\n",
      "Iteration 1159, loss = 0.00887440\n",
      "Iteration 1160, loss = 0.00885298\n",
      "Iteration 1161, loss = 0.00883677\n",
      "Iteration 1162, loss = 0.00883940\n",
      "Iteration 1163, loss = 0.00883558\n",
      "Iteration 1164, loss = 0.00882463\n",
      "Iteration 1165, loss = 0.00882164\n",
      "Iteration 1166, loss = 0.00878453\n",
      "Iteration 1167, loss = 0.00876468\n",
      "Iteration 1168, loss = 0.00875620\n",
      "Iteration 1169, loss = 0.00876562\n",
      "Iteration 1170, loss = 0.00874142\n",
      "Iteration 1171, loss = 0.00871758\n",
      "Iteration 1172, loss = 0.00874643\n",
      "Iteration 1173, loss = 0.00873145\n",
      "Iteration 1174, loss = 0.00872434\n",
      "Iteration 1175, loss = 0.00870407\n",
      "Iteration 1176, loss = 0.00871622\n",
      "Iteration 1177, loss = 0.00869226\n",
      "Iteration 1178, loss = 0.00869351\n",
      "Iteration 1179, loss = 0.00866338\n",
      "Iteration 1180, loss = 0.00864927\n",
      "Iteration 1181, loss = 0.00864055\n",
      "Iteration 1182, loss = 0.00864362\n",
      "Iteration 1183, loss = 0.00857602\n",
      "Iteration 1184, loss = 0.00861306\n",
      "Iteration 1185, loss = 0.00864220\n",
      "Iteration 1186, loss = 0.00859661\n",
      "Iteration 1187, loss = 0.00867337\n",
      "Iteration 1188, loss = 0.00857109\n",
      "Iteration 1189, loss = 0.00854928\n",
      "Iteration 1190, loss = 0.00854999\n",
      "Iteration 1191, loss = 0.00856470\n",
      "Iteration 1192, loss = 0.00854405\n",
      "Iteration 1193, loss = 0.00864084\n",
      "Iteration 1194, loss = 0.00853052\n",
      "Iteration 1195, loss = 0.00859189\n",
      "Iteration 1196, loss = 0.00861351\n",
      "Iteration 1197, loss = 0.00850585\n",
      "Iteration 1198, loss = 0.00848674\n",
      "Iteration 1199, loss = 0.00846362\n",
      "Iteration 1200, loss = 0.00844071\n",
      "Iteration 1201, loss = 0.00843569\n",
      "Iteration 1202, loss = 0.00845756\n",
      "Iteration 1203, loss = 0.00844645\n",
      "Iteration 1204, loss = 0.00843081\n",
      "Iteration 1205, loss = 0.00842696\n",
      "Iteration 1206, loss = 0.00844404\n",
      "Iteration 1207, loss = 0.00839225\n",
      "Iteration 1208, loss = 0.00839551\n",
      "Iteration 1209, loss = 0.00837249\n",
      "Iteration 1210, loss = 0.00836066\n",
      "Iteration 1211, loss = 0.00834554\n",
      "Iteration 1212, loss = 0.00838351\n",
      "Iteration 1213, loss = 0.00842104\n",
      "Iteration 1214, loss = 0.00835400\n",
      "Iteration 1215, loss = 0.00839280\n",
      "Iteration 1216, loss = 0.00831140\n",
      "Iteration 1217, loss = 0.00830687\n",
      "Iteration 1218, loss = 0.00830544\n",
      "Iteration 1219, loss = 0.00829527\n",
      "Iteration 1220, loss = 0.00835991\n",
      "Iteration 1221, loss = 0.00826621\n",
      "Iteration 1222, loss = 0.00824263\n",
      "Iteration 1223, loss = 0.00827676\n",
      "Iteration 1224, loss = 0.00830209\n",
      "Iteration 1225, loss = 0.00821659\n",
      "Iteration 1226, loss = 0.00824872\n",
      "Iteration 1227, loss = 0.00825071\n",
      "Iteration 1228, loss = 0.00822568\n",
      "Iteration 1229, loss = 0.00818645\n",
      "Iteration 1230, loss = 0.00820007\n",
      "Iteration 1231, loss = 0.00819574\n",
      "Iteration 1232, loss = 0.00818022\n",
      "Iteration 1233, loss = 0.00814989\n",
      "Iteration 1234, loss = 0.00818007\n",
      "Iteration 1235, loss = 0.00822130\n",
      "Iteration 1236, loss = 0.00812435\n",
      "Iteration 1237, loss = 0.00815426\n",
      "Iteration 1238, loss = 0.00811184\n",
      "Iteration 1239, loss = 0.00810475\n",
      "Iteration 1240, loss = 0.00811603\n",
      "Iteration 1241, loss = 0.00810161\n",
      "Iteration 1242, loss = 0.00812504\n",
      "Iteration 1243, loss = 0.00808414\n",
      "Iteration 1244, loss = 0.00806556\n",
      "Iteration 1245, loss = 0.00804200\n",
      "Iteration 1246, loss = 0.00806849\n",
      "Iteration 1247, loss = 0.00803135\n",
      "Iteration 1248, loss = 0.00803284\n",
      "Iteration 1249, loss = 0.00810623\n",
      "Iteration 1250, loss = 0.00800706\n",
      "Iteration 1251, loss = 0.00806103\n",
      "Iteration 1252, loss = 0.00800173\n",
      "Iteration 1253, loss = 0.00799543\n",
      "Iteration 1254, loss = 0.00800419\n",
      "Iteration 1255, loss = 0.00798609\n",
      "Iteration 1256, loss = 0.00799383\n",
      "Iteration 1257, loss = 0.00796760\n",
      "Iteration 1258, loss = 0.00793444\n",
      "Iteration 1259, loss = 0.00800872\n",
      "Iteration 1260, loss = 0.00793522\n",
      "Iteration 1261, loss = 0.00793306\n",
      "Iteration 1262, loss = 0.00794066\n",
      "Iteration 1263, loss = 0.00793525\n",
      "Iteration 1264, loss = 0.00793279\n",
      "Iteration 1265, loss = 0.00798113\n",
      "Iteration 1266, loss = 0.00792901\n",
      "Iteration 1267, loss = 0.00790170\n",
      "Iteration 1268, loss = 0.00791680\n",
      "Iteration 1269, loss = 0.00791941\n",
      "Iteration 1270, loss = 0.00787237\n",
      "Iteration 1271, loss = 0.00786431\n",
      "Iteration 1272, loss = 0.00783735\n",
      "Iteration 1273, loss = 0.00783868\n",
      "Iteration 1274, loss = 0.00785872\n",
      "Iteration 1275, loss = 0.00780748\n",
      "Iteration 1276, loss = 0.00780963\n",
      "Iteration 1277, loss = 0.00779221\n",
      "Iteration 1278, loss = 0.00781601\n",
      "Iteration 1279, loss = 0.00781931\n",
      "Iteration 1280, loss = 0.00781950\n",
      "Iteration 1281, loss = 0.00776054\n",
      "Iteration 1282, loss = 0.00775441\n",
      "Iteration 1283, loss = 0.00776850\n",
      "Iteration 1284, loss = 0.00774509\n",
      "Iteration 1285, loss = 0.00774644\n",
      "Iteration 1286, loss = 0.00775470\n",
      "Iteration 1287, loss = 0.00770399\n",
      "Iteration 1288, loss = 0.00776300\n",
      "Iteration 1289, loss = 0.00772153\n",
      "Iteration 1290, loss = 0.00776256\n",
      "Iteration 1291, loss = 0.00767407\n",
      "Iteration 1292, loss = 0.00772967\n",
      "Iteration 1293, loss = 0.00772560\n",
      "Iteration 1294, loss = 0.00770188\n",
      "Iteration 1295, loss = 0.00767576\n",
      "Iteration 1296, loss = 0.00763591\n",
      "Iteration 1297, loss = 0.00762605\n",
      "Iteration 1298, loss = 0.00766324\n",
      "Iteration 1299, loss = 0.00762061\n",
      "Iteration 1300, loss = 0.00764852\n",
      "Iteration 1301, loss = 0.00763353\n",
      "Iteration 1302, loss = 0.00760479\n",
      "Iteration 1303, loss = 0.00757289\n",
      "Iteration 1304, loss = 0.00757928\n",
      "Iteration 1305, loss = 0.00765864\n",
      "Iteration 1306, loss = 0.00758330\n",
      "Iteration 1307, loss = 0.00755255\n",
      "Iteration 1308, loss = 0.00758893\n",
      "Iteration 1309, loss = 0.00756283\n",
      "Iteration 1310, loss = 0.00755209\n",
      "Iteration 1311, loss = 0.00762769\n",
      "Iteration 1312, loss = 0.00755264\n",
      "Iteration 1313, loss = 0.00753272\n",
      "Iteration 1314, loss = 0.00753689\n",
      "Iteration 1315, loss = 0.00752167\n",
      "Iteration 1316, loss = 0.00748421\n",
      "Iteration 1317, loss = 0.00749816\n",
      "Iteration 1318, loss = 0.00750410\n",
      "Iteration 1319, loss = 0.00747387\n",
      "Iteration 1320, loss = 0.00746996\n",
      "Iteration 1321, loss = 0.00747929\n",
      "Iteration 1322, loss = 0.00751073\n",
      "Iteration 1323, loss = 0.00756761\n",
      "Iteration 1324, loss = 0.00742454\n",
      "Iteration 1325, loss = 0.00744391\n",
      "Iteration 1326, loss = 0.00742676\n",
      "Iteration 1327, loss = 0.00742238\n",
      "Iteration 1328, loss = 0.00742005\n",
      "Iteration 1329, loss = 0.00741332\n",
      "Iteration 1330, loss = 0.00739830\n",
      "Iteration 1331, loss = 0.00742970\n",
      "Iteration 1332, loss = 0.00738341\n",
      "Iteration 1333, loss = 0.00740539\n",
      "Iteration 1334, loss = 0.00738843\n",
      "Iteration 1335, loss = 0.00743106\n",
      "Iteration 1336, loss = 0.00738626\n",
      "Iteration 1337, loss = 0.00730505\n",
      "Iteration 1338, loss = 0.00744185\n",
      "Iteration 1339, loss = 0.00739906\n",
      "Iteration 1340, loss = 0.00735386\n",
      "Iteration 1341, loss = 0.00732980\n",
      "Iteration 1342, loss = 0.00731956\n",
      "Iteration 1343, loss = 0.00730376\n",
      "Iteration 1344, loss = 0.00732448\n",
      "Iteration 1345, loss = 0.00728566\n",
      "Iteration 1346, loss = 0.00728125\n",
      "Iteration 1347, loss = 0.00729404\n",
      "Iteration 1348, loss = 0.00726161\n",
      "Iteration 1349, loss = 0.00727158\n",
      "Iteration 1350, loss = 0.00724533\n",
      "Iteration 1351, loss = 0.00727123\n",
      "Iteration 1352, loss = 0.00724649\n",
      "Iteration 1353, loss = 0.00726475\n",
      "Iteration 1354, loss = 0.00724251\n",
      "Iteration 1355, loss = 0.00725808\n",
      "Iteration 1356, loss = 0.00722663\n",
      "Iteration 1357, loss = 0.00727268\n",
      "Iteration 1358, loss = 0.00719973\n",
      "Iteration 1359, loss = 0.00720486\n",
      "Iteration 1360, loss = 0.00722891\n",
      "Iteration 1361, loss = 0.00720320\n",
      "Iteration 1362, loss = 0.00718611\n",
      "Iteration 1363, loss = 0.00716318\n",
      "Iteration 1364, loss = 0.00717822\n",
      "Iteration 1365, loss = 0.00716409\n",
      "Iteration 1366, loss = 0.00717569\n",
      "Iteration 1367, loss = 0.00715007\n",
      "Iteration 1368, loss = 0.00721597\n",
      "Iteration 1369, loss = 0.00716167\n",
      "Iteration 1370, loss = 0.00711854\n",
      "Iteration 1371, loss = 0.00712580\n",
      "Iteration 1372, loss = 0.00709192\n",
      "Iteration 1373, loss = 0.00713915\n",
      "Iteration 1374, loss = 0.00709371\n",
      "Iteration 1375, loss = 0.00709681\n",
      "Iteration 1376, loss = 0.00708613\n",
      "Iteration 1377, loss = 0.00711709\n",
      "Iteration 1378, loss = 0.00709427\n",
      "Iteration 1379, loss = 0.00708656\n",
      "Iteration 1380, loss = 0.00710980\n",
      "Iteration 1381, loss = 0.00706572\n",
      "Iteration 1382, loss = 0.00707485\n",
      "Iteration 1383, loss = 0.00704496\n",
      "Iteration 1384, loss = 0.00704034\n",
      "Iteration 1385, loss = 0.00703760\n",
      "Iteration 1386, loss = 0.00702298\n",
      "Iteration 1387, loss = 0.00702971\n",
      "Iteration 1388, loss = 0.00704413\n",
      "Iteration 1389, loss = 0.00710018\n",
      "Iteration 1390, loss = 0.00701748\n",
      "Iteration 1391, loss = 0.00702113\n",
      "Iteration 1392, loss = 0.00701084\n",
      "Iteration 1393, loss = 0.00699283\n",
      "Iteration 1394, loss = 0.00700183\n",
      "Iteration 1395, loss = 0.00698506\n",
      "Iteration 1396, loss = 0.00698111\n",
      "Iteration 1397, loss = 0.00695605\n",
      "Iteration 1398, loss = 0.00695169\n",
      "Iteration 1399, loss = 0.00693411\n",
      "Iteration 1400, loss = 0.00693916\n",
      "Iteration 1401, loss = 0.00692488\n",
      "Iteration 1402, loss = 0.00696151\n",
      "Iteration 1403, loss = 0.00691524\n",
      "Iteration 1404, loss = 0.00690537\n",
      "Iteration 1405, loss = 0.00690472\n",
      "Iteration 1406, loss = 0.00689560\n",
      "Iteration 1407, loss = 0.00689990\n",
      "Iteration 1408, loss = 0.00688117\n",
      "Iteration 1409, loss = 0.00685809\n",
      "Iteration 1410, loss = 0.00687193\n",
      "Iteration 1411, loss = 0.00686613\n",
      "Iteration 1412, loss = 0.00684151\n",
      "Iteration 1413, loss = 0.00686640\n",
      "Iteration 1414, loss = 0.00682407\n",
      "Iteration 1415, loss = 0.00686652\n",
      "Iteration 1416, loss = 0.00682183\n",
      "Iteration 1417, loss = 0.00684022\n",
      "Iteration 1418, loss = 0.00680529\n",
      "Iteration 1419, loss = 0.00680498\n",
      "Iteration 1420, loss = 0.00681969\n",
      "Iteration 1421, loss = 0.00684196\n",
      "Iteration 1422, loss = 0.00677306\n",
      "Iteration 1423, loss = 0.00682228\n",
      "Iteration 1424, loss = 0.00677223\n",
      "Iteration 1425, loss = 0.00684811\n",
      "Iteration 1426, loss = 0.00681591\n",
      "Iteration 1427, loss = 0.00677528\n",
      "Iteration 1428, loss = 0.00680980\n",
      "Iteration 1429, loss = 0.00678406\n",
      "Iteration 1430, loss = 0.00679401\n",
      "Iteration 1431, loss = 0.00674394\n",
      "Iteration 1432, loss = 0.00673343\n",
      "Iteration 1433, loss = 0.00673633\n",
      "Iteration 1434, loss = 0.00672540\n",
      "Iteration 1435, loss = 0.00670586\n",
      "Iteration 1436, loss = 0.00673681\n",
      "Iteration 1437, loss = 0.00673148\n",
      "Iteration 1438, loss = 0.00668311\n",
      "Iteration 1439, loss = 0.00669781\n",
      "Iteration 1440, loss = 0.00670622\n",
      "Iteration 1441, loss = 0.00668187\n",
      "Iteration 1442, loss = 0.00674257\n",
      "Iteration 1443, loss = 0.00666736\n",
      "Iteration 1444, loss = 0.00668566\n",
      "Iteration 1445, loss = 0.00663589\n",
      "Iteration 1446, loss = 0.00668339\n",
      "Iteration 1447, loss = 0.00664850\n",
      "Iteration 1448, loss = 0.00667073\n",
      "Iteration 1449, loss = 0.00667869\n",
      "Iteration 1450, loss = 0.00660297\n",
      "Iteration 1451, loss = 0.00662167\n",
      "Iteration 1452, loss = 0.00663714\n",
      "Iteration 1453, loss = 0.00661249\n",
      "Iteration 1454, loss = 0.00658358\n",
      "Iteration 1455, loss = 0.00659784\n",
      "Iteration 1456, loss = 0.00659958\n",
      "Iteration 1457, loss = 0.00659664\n",
      "Iteration 1458, loss = 0.00658433\n",
      "Iteration 1459, loss = 0.00660006\n",
      "Iteration 1460, loss = 0.00656035\n",
      "Iteration 1461, loss = 0.00658818\n",
      "Iteration 1462, loss = 0.00664845\n",
      "Iteration 1463, loss = 0.00657763\n",
      "Iteration 1464, loss = 0.00653475\n",
      "Iteration 1465, loss = 0.00655420\n",
      "Iteration 1466, loss = 0.00652034\n",
      "Iteration 1467, loss = 0.00654508\n",
      "Iteration 1468, loss = 0.00659968\n",
      "Iteration 1469, loss = 0.00650784\n",
      "Iteration 1470, loss = 0.00650160\n",
      "Iteration 1471, loss = 0.00649063\n",
      "Iteration 1472, loss = 0.00650807\n",
      "Iteration 1473, loss = 0.00650664\n",
      "Iteration 1474, loss = 0.00650022\n",
      "Iteration 1475, loss = 0.00651755\n",
      "Iteration 1476, loss = 0.00650414\n",
      "Iteration 1477, loss = 0.00647556\n",
      "Iteration 1478, loss = 0.00654118\n",
      "Iteration 1479, loss = 0.00646925\n",
      "Iteration 1480, loss = 0.00648915\n",
      "Iteration 1481, loss = 0.00643980\n",
      "Iteration 1482, loss = 0.00641720\n",
      "Iteration 1483, loss = 0.00643149\n",
      "Iteration 1484, loss = 0.00645530\n",
      "Iteration 1485, loss = 0.00642730\n",
      "Iteration 1486, loss = 0.00641445\n",
      "Iteration 1487, loss = 0.00642373\n",
      "Iteration 1488, loss = 0.00645451\n",
      "Iteration 1489, loss = 0.00642045\n",
      "Iteration 1490, loss = 0.00641148\n",
      "Iteration 1491, loss = 0.00640398\n",
      "Iteration 1492, loss = 0.00637914\n",
      "Iteration 1493, loss = 0.00639395\n",
      "Iteration 1494, loss = 0.00636503\n",
      "Iteration 1495, loss = 0.00637172\n",
      "Iteration 1496, loss = 0.00633832\n",
      "Iteration 1497, loss = 0.00636962\n",
      "Iteration 1498, loss = 0.00640511\n",
      "Iteration 1499, loss = 0.00636803\n",
      "Iteration 1500, loss = 0.00646234\n",
      "Wall time: 7.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-09, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rede_neural_credit = MLPClassifier(max_iter = 1500, verbose=True, tol=0.00000000100, solver='adam', activation='relu', hidden_layer_sizes = (2,2))\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
